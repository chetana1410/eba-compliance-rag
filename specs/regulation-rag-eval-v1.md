# Regulation RAG Evaluation Framework 

This project provides an evaluation framework for Retrieval-Augmented Generation (RAG) implementations across regulations for the financial services industry. Multiple implementations can be added to the rag_implementations folder. The goal is to provide a common interface for evaluating RAG implementations, allowing for easy comparison and benchmarking.

## Implementation Notes
- The evaluation framework in `src/regulations_rag_eval/eval_framework` uses the types in `src/regulations_rag_eval/eval_framework_data_types.py` and when calling the RAG implementations, it uses the types in `src/regulations_rag_eval/rag_implementation_data_types.py`
- All RAG implementations in `src/regulations_rag_eval/rag_implementations` use the types in `src/regulations_rag_eval/rag_implementation_data_types.py`
- External libraries used:
  - aiofiles (for async file operations)
  - Click (for CLI interface)
  - Pandas (for data processing)
  - Pydantic (for data validation)
- Use `uv add <package>` to add libraries (we're using uv to manage the project)
- Add `regulation-rag-eval = "regulation_rag_eval:main"` to the project.scripts section in pyproject.toml

## Project Structure
- `data/` - Contains datasets and resources used for evaluation
    - `CRR.txt` - Full text of the Capital Requirements Regulation used by CAG implementation
    - `qa_datasets/`
        - `regulation_questions.json` - Questions dataset, structure is RegulationQuestion(TypedDict)
        - `golden_answers.json` - Golden answers dataset, structure is GoldenAnswer(TypedDict)
- `specs/` - Documentation and specifications for the project
    - `regulation-rag-eval-v1.md` - Main specification document for the evaluation framework
    - `future_work_stage_2.md` - Plans for future development
- `src/regulations_rag_eval/`
    - `eval_framework/`
        - `get_rag_impl_entry_point.py` - Dynamically imports RAG implementation entry points
            - `def get_rag_impl_entry_point(rag_implementation: str) -> Callable[[List[str]], List[RAGResult]]`
        - `rag_eval.py` - Main evaluation script for the RAG framework
            - `async def rag_eval(rag_implementation: str) -> None`
                - Loads questions from JSON file
                - Gets the entry point for the RAG implementation
                - Calls implementation and processes results
                - Saves generated answers to JSON file
    - `eval_framework_data_types.py` - Defines data types used in the evaluation framework
        - `RegulationQuestion` - Represents a question about regulations
        - `GoldenAnswer` - Represents a golden/reference answer
        - `GeneratedAnswer` - Represents an answer generated by a RAG implementation
        - `AnswerEvaluation` - Represents an evaluation of a generated answer
        - `RetrievalEvaluation` - Represents an evaluation of retrieved articles
        - `SampledAnswer` - Named tuple for sampled answers during evaluation
        - `AnswerEvaluationResult` - Named tuple for evaluation results
    - `evaluate_answers.py` - Evaluates generated answers against golden answers
        - `async def evaluate_answers(answers: List[SampledAnswer], model: str) -> List[AnswerEvaluationResult]`
        - `async def evaluate(generated_answers, regulation_questions, golden_answers, model) -> List[AnswerEvaluation]`
        - `async def generate_evaluate_with_files(answers_file_path, questions_file_path, golden_answer_file_path, evaluation_output_file_path, model) -> None`
    - `llm_as_judge_prompt.py` - Generates prompts for LLMs to evaluate answers
        - `def llm_as_judge_prompt(question, golden_answer, answer) -> str`
    - `open_router_request.py` - Makes API requests to OpenRouter
    - `rag_implementation_data_types.py` - Defines data types used by RAG implementations
        - `RAGResult` - Named tuple for RAG generation results (answer and retrieved articles)
    - `rag_implementations/` - Contains various RAG implementations
        - `stub/` - Simple implementation that returns "I don't know" for all questions
            - `generate_answers.py`
                - `async def generate_answers(questions: List[str]) -> List[RAGResult]`
        - `CAG/` - Cache Augmented Generation implementation
            - `generate_answers.py`
                - `async def generate_answers(questions: List[str], model: str) -> List[RAGResult]`
        - `cag_prompt.py` - Creates prompts for Cache Augmented Generation approach
            - `def create_cag_prompt(question: str, context: str) -> str`
    - `short_hash.py` - Generates short hashes for questions and answers
    - `utils.py` - Utility functions
        - `def prompt_escape(s: str) -> str` - Escapes strings for use in prompts

